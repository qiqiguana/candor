/*
 * This file was automatically generated by EvoSuite
 */

package weka.classifiers.rules;

import org.junit.Test;
import org.junit.runner.RunWith;
import org.evosuite.junit.EvoSuiteRunner;
import static org.junit.Assert.*;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.List;
import org.junit.BeforeClass;
import weka.classifiers.rules.JRip;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.SparseInstance;
import weka.core.UnassignedClassException;

@RunWith(EvoSuiteRunner.class)
public class JRipEvoSuiteTest {

  @BeforeClass 
  public static void initEvoSuiteFramework(){ 
    org.evosuite.Properties.REPLACE_CALLS = true; 
  } 


  @Test
  public void test0()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("", "");
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd(attribute0);
      double double0 = jRip_NumericAntd0.getCover();
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccuRate(), 0.01D);
      assertEquals(Double.NaN, jRip_NumericAntd0.getAttrValue(), 0.01D);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, jRip0.getFolds());
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccu(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(" <= NaN", jRip_NumericAntd0.toString());
      assertEquals(0.0, jRip_NumericAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, jRip0.getCheckErrorRate());
  }

  @Test
  public void test1()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("");
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      assertNotNull(jRip_NominalAntd0);
      
      double double0 = jRip_NominalAntd0.getAccuRate();
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccu(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(false, jRip0.getDebug());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd0.getCover(), 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals(0.0, jRip_NominalAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd0.getAttrValue(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
  }

  @Test
  public void test2()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("");
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      assertNotNull(jRip_NominalAntd0);
      
      double double0 = jRip_NominalAntd0.getAccu();
      assertEquals(Double.NaN, jRip_NominalAntd0.getCover(), 0.01D);
      assertEquals(0.0, jRip_NominalAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccuRate(), 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd0.getAttrValue(), 0.01D);
  }

  @Test
  public void test3()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("");
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      assertNotNull(jRip_NominalAntd0);
      
      double double0 = jRip_NominalAntd0.getAttrValue();
      assertEquals(3, jRip0.getFolds());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(0.0, jRip_NominalAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccuRate(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd0.getCover(), 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccu(), 0.01D);
  }

  @Test
  public void test4()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n", 65);
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd(attribute0);
      double double0 = jRip_NumericAntd0.getMaxInfoGain();
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(0.0, double0, 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccuRate(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(Double.NaN, jRip_NumericAntd0.getCover(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccu(), 0.01D);
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(Double.NaN, jRip_NumericAntd0.getAttrValue(), 0.01D);
      assertEquals("This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n <= NaN", jRip_NumericAntd0.toString());
      assertEquals(false, jRip0.getDebug());
  }

  @Test
  public void test5()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("", "");
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd(attribute0);
      JRip.NumericAntd jRip_NumericAntd1 = (JRip.NumericAntd)jRip_NumericAntd0.copy();
      assertEquals(" <= NaN", jRip_NumericAntd1.toString());
      assertEquals(Double.NaN, jRip_NumericAntd1.getAccu(), 0.01D);
      assertNotSame(jRip_NumericAntd1, jRip_NumericAntd0);
      assertEquals(false, jRip0.getDebug());
      assertEquals(Double.NaN, jRip_NumericAntd1.getAccuRate(), 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals(Double.NaN, jRip_NumericAntd1.getCover(), 0.01D);
      assertEquals(Double.NaN, jRip_NumericAntd1.getAttrValue(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(0.0, jRip_NumericAntd1.getMaxInfoGain(), 0.01D);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
  }

  @Test
  public void test6()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("");
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      assertNotNull(jRip_NominalAntd0);
      
      JRip.NominalAntd jRip_NominalAntd1 = (JRip.NominalAntd)jRip_NominalAntd0.copy();
      assertEquals(3, jRip0.getFolds());
      assertEquals(false, jRip0.getDebug());
      assertEquals(2, jRip0.getOptimizations());
      assertNotNull(jRip_NominalAntd1);
      assertEquals(Double.NaN, jRip_NominalAntd1.getAttrValue(), 0.01D);
      assertEquals(Double.NaN, jRip_NominalAntd1.getCover(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(Double.NaN, jRip_NominalAntd1.getAccuRate(), 0.01D);
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(Double.NaN, jRip_NominalAntd1.getAccu(), 0.01D);
      assertEquals(0.0, jRip_NominalAntd1.getMaxInfoGain(), 0.01D);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
  }

  @Test
  public void test7()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("/{u.Xr)", (List<String>) null, 0);
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      assertNotNull(jRip_NominalAntd0);
      
      String string0 = jRip_NominalAntd0.getRevision();
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccuRate(), 0.01D);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertNotNull(string0);
      assertEquals("8118", string0);
      assertEquals(Double.NaN, jRip_NominalAntd0.getCover(), 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(0.0, jRip_NominalAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccu(), 0.01D);
      assertEquals(false, jRip0.getDebug());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(Double.NaN, jRip_NominalAntd0.getAttrValue(), 0.01D);
      assertEquals(3, jRip0.getFolds());
  }

  @Test
  public void test8()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("", 185);
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      assertNotNull(jRip_NominalAntd0);
      
      String string0 = jRip_NominalAntd0.toString();
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccuRate(), 0.01D);
      assertNotNull(string0);
      assertEquals(Double.NaN, jRip_NominalAntd0.getAttrValue(), 0.01D);
      assertEquals(" = ", string0);
      assertEquals(Double.NaN, jRip_NominalAntd0.getAccu(), 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(false, jRip0.getDebug());
      assertEquals(0.0, jRip_NominalAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(Double.NaN, jRip_NominalAntd0.getCover(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test9()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.checkErrorRateTipText();
      assertEquals(3, jRip0.getFolds());
      assertEquals(1L, jRip0.getSeed());
      assertEquals("Whether check for error rate >= 1/2 is included in stopping criterion.", string0);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(false, jRip0.getDebug());
  }

  @Test
  public void test10()  throws Throwable  {
      JRip jRip0 = new JRip();
      boolean boolean0 = jRip0.getUsePruning();
      assertEquals(false, jRip0.getDebug());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(3, jRip0.getFolds());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, boolean0);
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test11()  throws Throwable  {
      JRip jRip0 = new JRip();
      double double0 = jRip0.getMinNo();
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2.0, double0, 0.01D);
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, jRip0.getFolds());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test12()  throws Throwable  {
      JRip jRip0 = new JRip();
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      
      jRip0.setMinNo(0.0);
      assertEquals(3, jRip0.getFolds());
  }

  @Test
  public void test13()  throws Throwable  {
      JRip jRip0 = new JRip();
      assertEquals(3, jRip0.getFolds());
      
      jRip0.setFolds(0);
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
  }

  @Test
  public void test14()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.usePruningTipText();
      assertEquals("Whether pruning is performed.", string0);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(3, jRip0.getFolds());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, jRip0.getUsePruning());
  }

  @Test
  public void test15()  throws Throwable  {
      JRip jRip0 = new JRip();
      boolean boolean0 = jRip0.getDebug();
      assertEquals(false, boolean0);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(3, jRip0.getFolds());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
  }

  @Test
  public void test16()  throws Throwable  {
      JRip jRip0 = new JRip();
      boolean boolean0 = jRip0.getCheckErrorRate();
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, boolean0);
      assertEquals(false, jRip0.getDebug());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(3, jRip0.getFolds());
      assertEquals(true, jRip0.getUsePruning());
  }

  @Test
  public void test17()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.getRevision();
      assertEquals(3, jRip0.getFolds());
      assertNotNull(string0);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(false, jRip0.getDebug());
      assertEquals("8118", string0);
  }

  @Test
  public void test18()  throws Throwable  {
      JRip jRip0 = new JRip();
      Enumeration<Object> enumeration0 = jRip0.enumerateMeasures();
      assertEquals(false, jRip0.getDebug());
      assertEquals(3, jRip0.getFolds());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertNotNull(enumeration0);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, enumeration0.hasMoreElements());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test19()  throws Throwable  {
      JRip jRip0 = new JRip();
      assertEquals(true, jRip0.getUsePruning());
      
      jRip0.setUsePruning(false);
      jRip0.getOptions();
      assertEquals(false, jRip0.getUsePruning());
      assertEquals(1L, jRip0.getSeed());
  }

  @Test
  public void test20()  throws Throwable  {
      JRip jRip0 = new JRip();
      jRip0.setDebug(true);
      jRip0.setUsePruning(false);
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("8118", arrayList0, 25);
      try {
        jRip0.rulesetForOneClass(318.6825054050287, instances0, 318.6825054050287, (double) 25);
        fail("Expecting exception: NullPointerException");
      } catch(NullPointerException e) {
      }
  }

  @Test
  public void test21()  throws Throwable  {
      JRip jRip0 = new JRip();
      long long0 = jRip0.getSeed();
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, jRip0.getFolds());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(false, jRip0.getDebug());
      assertEquals(1L, long0);
  }

  @Test
  public void test22()  throws Throwable  {
      JRip jRip0 = new JRip();
      jRip0.setSeed(6L);
      assertEquals(6L, jRip0.getSeed());
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test23()  throws Throwable  {
      JRip jRip0 = new JRip();
      try {
        jRip0.buildClassifier((Instances) null);
        fail("Expecting exception: NullPointerException");
      } catch(NullPointerException e) {
      }
  }

  @Test
  public void test24()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.globalInfo();
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(3, jRip0.getFolds());
      assertEquals(false, jRip0.getDebug());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals("This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n", string0);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(2, jRip0.getOptimizations());
      assertNotNull(string0);
  }

  @Test
  public void test25()  throws Throwable  {
      JRip jRip0 = new JRip();
      jRip0.setOptimizations((-1403));
      assertEquals(-1403, jRip0.getOptimizations());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
  }

  @Test
  public void test26()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.minNoTipText();
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, jRip0.getFolds());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(false, jRip0.getDebug());
      assertEquals("The minimum total weight of the instances in a rule.", string0);
  }

  @Test
  public void test27()  throws Throwable  {
      JRip jRip0 = new JRip();
      // Undeclared exception!
      try {
        jRip0.getRuleStats(109);
        fail("Expecting exception: NullPointerException");
      } catch(NullPointerException e) {
      }
  }

  @Test
  public void test28()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.optimizationsTipText();
      assertEquals(false, jRip0.getDebug());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(3, jRip0.getFolds());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals("The number of optimization runs.", string0);
      assertEquals(true, jRip0.getUsePruning());
  }

  @Test
  public void test29()  throws Throwable  {
      JRip jRip0 = new JRip();
      assertEquals(true, jRip0.getCheckErrorRate());
      
      jRip0.setCheckErrorRate(false);
      String[] stringArray0 = jRip0.getOptions();
      jRip0.setOptions(stringArray0);
      assertEquals(false, jRip0.getCheckErrorRate());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
  }

  @Test
  public void test30()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.foldsTipText();
      assertEquals(3, jRip0.getFolds());
      assertEquals("Determines the amount of data used for pruning. One fold is used for pruning, the rest for growing the rules.", string0);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(false, jRip0.getDebug());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, jRip0.getCheckErrorRate());
  }

  @Test
  public void test31()  throws Throwable  {
      JRip jRip0 = new JRip();
      int int0 = jRip0.getOptimizations();
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(3, jRip0.getFolds());
      assertEquals(2, int0);
  }

  @Test
  public void test32()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.debugTipText();
      assertEquals(false, jRip0.getDebug());
      assertEquals(1L, jRip0.getSeed());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(3, jRip0.getFolds());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals("Whether debug information is output to the console.", string0);
  }

  @Test
  public void test33()  throws Throwable  {
      JRip jRip0 = new JRip();
      String string0 = jRip0.seedTipText();
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(false, jRip0.getDebug());
      assertEquals("The seed used for randomizing the data.", string0);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(3, jRip0.getFolds());
      assertEquals(1L, jRip0.getSeed());
  }

  @Test
  public void test34()  throws Throwable  {
      JRip jRip0 = new JRip();
      int int0 = jRip0.getFolds();
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(false, jRip0.getDebug());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, int0);
  }

  @Test
  public void test35()  throws Throwable  {
      JRip jRip0 = new JRip();
      Enumeration<Object> enumeration0 = jRip0.listOptions();
      assertEquals(1L, jRip0.getSeed());
      assertNotNull(enumeration0);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, jRip0.getFolds());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(false, jRip0.getDebug());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test36()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd((Attribute) null);
      String string0 = jRip_NumericAntd0.getRevision();
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(false, jRip0.getDebug());
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccu(), 0.01D);
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccuRate(), 0.01D);
      assertEquals(0.0, jRip_NumericAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals("8118", string0);
      assertEquals(Double.NaN, jRip_NumericAntd0.getAttrValue(), 0.01D);
      assertNotNull(string0);
      assertEquals(Double.NaN, jRip_NumericAntd0.getCover(), 0.01D);
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(Double.NaN, jRip_NumericAntd0.getSplitPoint(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(true, jRip0.getUsePruning());
  }

  @Test
  public void test37()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd((Attribute) null);
      double double0 = jRip_NumericAntd0.getSplitPoint();
      assertEquals(0.0, jRip_NumericAntd0.getMaxInfoGain(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertEquals(false, jRip0.getDebug());
      assertEquals(2, jRip0.getOptimizations());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(3, jRip0.getFolds());
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccuRate(), 0.01D);
      assertEquals(Double.NaN, jRip_NumericAntd0.getAttrValue(), 0.01D);
      assertEquals(Double.NaN, double0, 0.01D);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(Double.NaN, jRip_NumericAntd0.getAccu(), 0.01D);
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(Double.NaN, jRip_NumericAntd0.getCover(), 0.01D);
  }

  @Test
  public void test38()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      JRip.RipperRule jRip_RipperRule1 = (JRip.RipperRule)jRip_RipperRule0.copy();
      assertEquals((-1.0), jRip_RipperRule0.getConsequent(), 0.01D);
      assertNotSame(jRip_RipperRule0, jRip_RipperRule1);
      assertEquals(false, jRip0.getDebug());
      assertEquals((-1.0), jRip_RipperRule1.getConsequent(), 0.01D);
      assertEquals(1L, jRip0.getSeed());
      assertNotNull(jRip_RipperRule1);
      assertEquals(true, jRip0.getUsePruning());
      assertEquals(3, jRip0.getFolds());
      assertEquals(true, jRip0.getCheckErrorRate());
      assertEquals(2.0, jRip0.getMinNo(), 0.01D);
      assertEquals(2, jRip0.getOptimizations());
  }

  @Test
  public void test39()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      jRip_RipperRule0.size();
  }

  @Test
  public void test40()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      jRip_RipperRule0.getRevision();
  }

  @Test
  public void test41()  throws Throwable  {
      JRip jRip0 = new JRip();
      String[] stringArray0 = new String[6];
      stringArray0[0] = "This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n";
      stringArray0[1] = "This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n";
      stringArray0[2] = "This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n";
      stringArray0[3] = "This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n";
      stringArray0[4] = "This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n";
      stringArray0[5] = "This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n";
      jRip0.setOptions(stringArray0);
  }

  @Test
  public void test42()  throws Throwable  {
      JRip jRip0 = new JRip();
      String[] stringArray0 = jRip0.getOptions();
      jRip0.setOptions(stringArray0);
  }

  @Test
  public void test43()  throws Throwable  {
      JRip jRip0 = new JRip();
      // Undeclared exception!
      try {
        jRip0.getMeasure("\nThe probability of a word given the class\n-----------------------------------------\n\t");
        fail("Expecting exception: IllegalArgumentException");
      } catch(IllegalArgumentException e) {
        /*
         * 
         * The probability of a word given the class
         * -----------------------------------------
         * \t not supported (RIPPER)
         */
      }
  }

  @Test
  public void test44()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("", "");
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd(attribute0);
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, 0);
      jRip_NumericAntd0.splitData(instances0, (double) 0, Double.NaN);
  }

  @Test
  public void test45()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("2");
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd(attribute0);
      double[] doubleArray0 = new double[1];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-41.667818828064966), doubleArray0);
      jRip_NumericAntd0.covers((Instance) binarySparseInstance0);
  }

  @Test
  public void test46()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("This class implements a propositional rule learner, Repeated Incremental Pruning to Produce Error Reduction (RIPPER), which was proposed by William W. Cohen as an optimized version of IREP. \n\nThe algorithm is briefly described as follows: \n\nInitialize RS = {}, and for each class from the less prevalent one to the more frequent one, DO: \n\n1. Building stage:\nRepeat 1.1 and 1.2 until the descrition length (DL) of the ruleset and examples is 64 bits greater than the smallest DL met so far, or there are no positive examples, or the error rate >= 50%. \n\n1.1. Grow phase:\nGrow one rule by greedily adding antecedents (or conditions) to the rule until the rule is perfect (i.e. 100% accurate).  The procedure tries every possible value of each attribute and selects the condition with highest information gain: p(log(p/t)-log(P/T)).\n\n1.2. Prune phase:\nIncrementally prune each rule and allow the pruning of any final sequences of the antecedents;The pruning metric is (p-n)/(p+n) -- but it's actually 2p/(p+n) -1, so in this implementation we simply use p/(p+n) (actually (p+1)/(p+n+2), thus if p+n is 0, it's 0.5).\n\n2. Optimization stage:\n after generating the initial ruleset {Ri}, generate and prune two variants of each rule Ri from randomized data using procedure 1.1 and 1.2. But one variant is generated from an empty rule while the other is generated by greedily adding antecedents to the original rule. Moreover, the pruning metric used here is (TP+TN)/(P+N).Then the smallest possible DL for each variant and the original rule is computed.  The variant with the minimal DL is selected as the final representative of Ri in the ruleset.After all the rules in {Ri} have been examined and if there are still residual positives, more rules are generated based on the residual positives using Building Stage again. \n3. Delete the rules from the ruleset that would increase the DL of the whole ruleset if it were in it. and add resultant ruleset to RS. \nENDDO\n\nNote that there seem to be 2 bugs in the original ripper program that would affect the ruleset size and accuracy slightly.  This implementation avoids these bugs and thus is a little bit different from Cohen's original implementation. Even after fixing the bugs, since the order of classes with the same frequency is not defined in ripper, there still seems to be some trivial difference between this implementation and the original ripper, especially for audiology data in UCI repository, where there are lots of classes of few instances.\n\nDetails please see:\n\nWilliam W. Cohen: Fast Effective Rule Induction. In: Twelfth International Conference on Machine Learning, 115-123, 1995.\n\nPS.  We have compared this implementation with the original ripper implementation in aspects of accuracy, ruleset size and running time on both artificial data \"ab+bcd+defg\" and UCI datasets.  In all these aspects it seems to be quite comparable to the original ripper implementation.  However, we didn't consider memory consumption optimization in this implementation.\n\n", 65);
      JRip.NumericAntd jRip_NumericAntd0 = jRip0.new NumericAntd(attribute0);
      jRip_NumericAntd0.toString();
  }

  @Test
  public void test47()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("", "");
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, 0);
      jRip_NominalAntd0.splitData(instances0, (double) 0, Double.NaN);
  }

  @Test
  public void test48()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("");
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      double[] doubleArray0 = new double[39];
      SparseInstance sparseInstance0 = new SparseInstance((double) 3, doubleArray0);
      jRip_NominalAntd0.covers((Instance) sparseInstance0);
  }

  @Test
  public void test49()  throws Throwable  {
      JRip jRip0 = new JRip();
      Attribute attribute0 = new Attribute("/{u.Xr)", (List<String>) null, 0);
      JRip.NominalAntd jRip_NominalAntd0 = jRip0.new NominalAntd(attribute0);
      int[] intArray0 = new int[9];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, 0);
      jRip_NominalAntd0.covers((Instance) binarySparseInstance0);
  }

  @Test
  public void test50()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      jRip_RipperRule0.covers((Instance) null);
  }

  @Test
  public void test51()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      jRip_RipperRule0.hasAntds();
  }

  @Test
  public void test52()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, 0);
      int[] intArray0 = new int[5];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.7, intArray0, 0);
      instances0.add((Instance) binarySparseInstance0);
      // Undeclared exception!
      try {
        jRip_RipperRule0.prune(instances0, true);
        fail("Expecting exception: UnassignedClassException");
      } catch(UnassignedClassException e) {
        /*
         * Class is not set!
         */
      }
  }

  @Test
  public void test53()  throws Throwable  {
      JRip jRip0 = new JRip();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      jRip0.setUsePruning(false);
      Instances instances0 = new Instances("", arrayList0, 41);
      try {
        jRip0.rulesetForOneClass(318.6825054050287, instances0, 318.6825054050287, (double) 41);
        fail("Expecting exception: UnassignedClassException");
      } catch(UnassignedClassException e) {
        /*
         * Class index is negative (not set)!
         */
      }
  }

  @Test
  public void test54()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      try {
        jRip_RipperRule0.grow((Instances) null);
        fail("Expecting exception: Exception");
      } catch(Exception e) {
        /*
         *  Consequent not set yet.
         */
      }
  }

  @Test
  public void test55()  throws Throwable  {
      JRip jRip0 = new JRip();
      JRip.RipperRule jRip_RipperRule0 = jRip0.new RipperRule();
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      Instances instances0 = new Instances("", arrayList0, 0);
      jRip_RipperRule0.prune(instances0, true);
  }

  @Test
  public void test56()  throws Throwable  {
      JRip jRip0 = new JRip();
      try {
        jRip0.rulesetForOneClass((double) (-1403), (Instances) null, (double) (-1403), (double) (-1403));
        fail("Expecting exception: NullPointerException");
      } catch(NullPointerException e) {
      }
  }

  @Test
  public void test57()  throws Throwable  {
      JRip jRip0 = new JRip();
      jRip0.toString();
  }
}
